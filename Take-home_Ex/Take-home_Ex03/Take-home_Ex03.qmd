---
title: "Take-home_Ex03: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods"
---

# Introduction

## Housing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

## Conventional, housing resale prices predictive models were built by using [**Ordinary Least Square (OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced for calibrating predictive model for housing resale prices.

# Data

#### The following data will be used in the analysis.

## Aspatial

##### Resale Flat Prices based on Registration Date \| Format: .xlsx \| Data.gov.sg

Resale Flat Prices based on Approval Date \| Format: .xlsx \| Data.gov.sg

## Geospatial

##### MRT Station Locations \| Format: Shapefile \| [LTA Data Mall](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=mrt) Bus Stop Locations \| Format: Shapefile \| [LTA Data Mall](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=mrt) MPSZ-19 \| Format: Shapefile \| In-class Materials Supermarkets \| Format: geojson \| [data.gov.sg](https://dataportal.asia/dataset/192501037_supermarkets/resource/3933d0d7-a795-4c41-b611-e0e4f4697e54) Elder Care Services Locations \| Format: Shapefile \| [data.gov.sg](https://data.gov.sg/dataset/eldercare-services) Child Care \| Format: geojson \| [data.gov.sg](https://dataportal.asia/dataset/203030733_child-care-services/resource/d5d984bc-331b-4865-9c43-29588f7f78ff) Hawker Centre Locations \| Format: geojson \| [data.gov.sg](https://data.gov.sg/dataset/hawker-centres) Pre-school Locations \| Format: Primary Schools \| Format: geojson \| [data.gov.sg](https://dataportal.asia/dataset/203030733_pre-schools-location) Shopping Mall \| Format: csv \| [Mall Coordinates Web Scraper](https://github.com/ValaryLim/Mall-Coordinates-Web-Scraper) CHAS Clinic \| Format: geojson \| [data.gov.sg](https://dataportal.asia/dataset/192501037_chas-clinics) Parks \| Format: shp \| [data.gov.sg](https://dataportal.asia/dataset/192521342_parks/resource/0fe1f447-9964-47a8-b802-f97fa8fed6a0)

# Install and load packages

#### **The following packages will be used:**

-   **tidyverse** for manipulating and tidying data

-   **dplyr** for wrangling and transforming data

-   **olsrr** for building OLS and performing diagnostics tests

-   **corrplot** for multivariate data visualisation and analysis

-   **ggpubr** for creating and customising ggplot2 based publication ready plots

-   **sf** for importing, managing and processing geospatial data

-   **spdep** for creating spatial weight matrix objects, global and spatial autocorrelation statistics and related calculation

-   **tmap** for choropleth mapping

-   httr

-   jsonlite

-   SpatialML

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, tmap, tidyverse, httr, jsonlite, GWmodel, SpatialML)
```

## Importing aspatial data

```{r}
resale <- read_csv("data/aspatial/resale-flat-prices.csv")
```

## Data Processing

### Filter

```{r}
resale <- resale %>%
  filter(flat_type == "4 ROOM")
```

### Transform

#### 1) Create new columns using mutate()

##### Referenced from [Senior Aisyah](https://aisyahajit2018-is415.netlify.app/posts/2021-11-07-take-home-exercise-3/)

-   address: concatenating block and street_name together using paste() function

-   remaining_lease_yr & remaining_lease_mth: splitting the year and months of the remaining_lease column using str_sub() and then converting the characters into integers using as.integer()

```{r}
resale_transform <- resale %>%
  mutate(resale, address = paste(block,street_name)) %>%
  mutate(resale, remaining_lease_yr = as.integer(str_sub(remaining_lease, 0, 2))) %>%
  mutate(resale, remaining_lease_mth = as.integer(str_sub(remaining_lease, 9, 11)))
```

##### Note: 2 new columns added

#### 2) Sum up remaining lease in months

-   Replace NA values in remaining_lease_mth with the value 0
-   Multiply remaining_lease_yr by 12 to convert it to months unit
-   Create remaining_lease_mths column
-   Select required columns for analysis

```{r}
resale_transform$remaining_lease_mth[is.na(resale_transform$remaining_lease_mth)] <- 0
resale_transform$remaining_lease_yr <- resale_transform$remaining_lease_yr * 12
resale_transform <- resale_transform %>% 
  mutate(resale_transform, remaining_lease_mths = rowSums(resale_transform[, c("remaining_lease_yr", "remaining_lease_mth")])) %>%
  select(month, town, address, block, street_name, flat_type, storey_range, floor_area_sqm, flat_model, 
         lease_commence_date, remaining_lease_mths, resale_price)
```

#### 3) Retrieve postal codes and coordinates of addresses

-   Required for getting proximity to locational factors

##### 3.1 - Create a list storing unique addresses

We create a list to store unique addresses to ensure that we do not run the GET request more than what is necessary We can also sort it to make it easier for us to see at which address the GET request will fail. Here, we use unique() function of base R package to extract the unique addresses then use sort() function of base R package to sort the unique vector.

```{r}
add_list <- sort(unique(resale_transform$address))
```

##### 3.2 - Create function to retrieve coordinates from OneMap.Sg API

Step 1: Create dataframe called retrieve_coords to store all final retrieved coordinates

Step 2: use GET() function of httr package to make a GET request to https://developers.onemap.sg/commonapi/search

-   OneMap SG offers functions for us to query spatial data from the API in a tidy format and provides additional functionalities to allow easy data manipulation.
-   Here, we will be using their REST APIs to search address data for a given search value and retrieve the coordinates of the searched location.
-   The required variables to be included in the GET request is as follows:\
    **searchVal:** Keywords entered by user that is used to filter out the results.\
    **returnGeom {Y/N}:** Checks if user wants to return the geometry.\
    **getAddrDetails {Y/N}:** Checks if user wants to return address details for a point.

Step 3: Create dataframe called new_row to store each final set of coordinates retrieved during the loop. Step 4: Check number of responses returned and append to main dataframe accordingly. Step 5: Append the returned response (new_row) to main dataframe (retrieved_coords).

```{r}
  get_coords <- function(add_list){
  
  # Create a data frame to store all retrieved coordinates
  retrieve_coords <- data.frame()
    
  for (i in add_list){
    #print(i)

    r <- GET('https://developers.onemap.sg/commonapi/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
      }
      
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE
        lng <- top1$LONGITUDE
        new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
      }
    }

    else {
      new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
    }
    
    # Add the row
    retrieve_coords <- rbind(retrieve_coords, new_row)
  }
  return(retrieve_coords)
}
```

#### 3.3 - Call function to retrieve coordinates

```{r}
coords <- get_coords(add_list)
```

#### 3.4 - Inspect results

Check whether the relevant columns contains any NA values

```{r}
coords[(is.na(coords$postal) | is.na(coords$latitude) | is.na(coords$longitude) | coords$postal=="NIL"), ]
```

-   Note: 215 and 216 Choa Chu Kang Central has no postal code, only coordinates. When researched further, according to gothere.sg website, it seems like these 2 addresses have their respective postal codes:
    -   680215
    -   680216
-   However, as OneMapAPISG returned the same set of coordinates for both of these addresses, we shall proceed with keeping them as we are more interested in the coordinates for our analysis later on.

#### 3.5 - Combining resale and coordinates data

```{r}
rs_coords <- left_join(resale_transform, coords, by = c('address' = 'address'))
```

### Write RDS File

-   As our subset resale dataset is now complete with the coordinates, we can now save it as an rds file.
-   This also helps us to prevent running the GET request more than what is needed.

```{r}
rs_coords_rds <- write_rds(rs_coords, "data/rds/rs_coords.rds")
```

### Import RDS File

```{r}
rs_coords <- read_rds("data/rds/rs_coords.rds")
glimpse(rs_coords)
```

### Assign, transform and check CRS

-   Since the coordinate columns are Latitude & Longitude which are in decimal degrees, the projected CRS will be WGS84.

-   We will need to assign them the respective EPSG code 4326 first before transforming it to 3414 which is the EPSG code for SVY21.

```{r}
rs_coords_sf <- st_as_sf(rs_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

```{r}
st_crs(rs_coords_sf)
```

### Check for invalid geometries

```{r}
length(which(st_is_valid(rs_coords_sf) == FALSE))
```

### Plot HDB resale points

```{r}
tmap_mode("view")
tm_shape(rs_coords_sf)+
  tm_dots(col="blue", size = 0.02)
tmap_mode("plot")
```

## Results {.tabset}

### Import Data

### Locational Data with Geographical Coordinates

```{r}
eldercare <- st_read(dsn = "data/geospatial/eldercare", layer = "ELDERCARE")
mrt <- read_csv("data/geospatial/TrainStation/mrtsg.csv")
supermarkets <- st_read("data/geospatial/supermarkets/supermarkets.geojson")
pre_school <- st_read("data/geospatial/pre-school/preschools-location.geojson")
parks <- st_read("data/geospatial/parks", layer = "NATIONALPARKS")
hawker <- st_read("data/geospatial/HawkerCentre/hawker-centres-geojson.geojson")
childcare <- st_read("data/geospatial/childcare/childcare.geojson")
clinic <- st_read("data/geospatial/CHAS_clinic/moh-chas-clinics.geojson")
bus <- st_read("data/geospatial/BusStop", layer = "BusStop")
```

```{r}
shopping_malls <- read_csv("data/geospatial/shoppingmall/mall_coordinates_updated.csv")
mall_sf <- st_as_sf(shopping_malls, coords = c("longitude", "latitude"), crs=4326)
```

```{r}
mrt <- st_as_sf(mrt, coords = c("Longitude", "Latitude"), crs=4326)
```

### Check CRS

```{r}
st_crs(eldercare)
st_crs(mrt)
st_crs(supermarkets)
st_crs(pre_school)
st_crs(parks)
st_crs(hawker)
st_crs(childcare)
st_crs(clinic)
st_crs(bus)
st_crs(mall_sf)
```

##### Note: Since hawker, supermarkets, clinic, childcare, kindergarten, malls_sf are WGS84, we have to transform CRS into SVY21.

##### Note: eldercare, mrt, bus, parks has EPSG code of 9001, we also need to change to 3414.

```{r}
eldercare <- st_set_crs(eldercare, 3414)
bus <- st_set_crs(bus, 3414)
parks <- st_set_crs(parks, 3414)

mrt <- mrt %>%
  st_transform(crs=3414)
hawker <- hawker %>%
  st_transform(crs=3414)
supermarkets <- supermarkets %>%
  st_transform(crs=3414)
clinic <- clinic %>%
  st_transform(crs=3414)
childcare <- childcare %>%
  st_transform(crs=3414)
pre_school <- pre_school %>%
  st_transform(crs=3414)
mall_sf <- st_transform(mall_sf, crs=3414)
```

### Check again

```{r}
st_crs(eldercare)
st_crs(mrt)
st_crs(supermarkets)
st_crs(pre_school)
st_crs(parks)
st_crs(hawker)
st_crs(childcare)
st_crs(clinic)
st_crs(bus)
st_crs(mall_sf)
```

```{r}
mrt
```

### Check for invalid geometries

```{r}
length(which(st_is_valid(mrt) == FALSE))
length(which(st_is_valid(hawker) == FALSE))
length(which(st_is_valid(parks) == FALSE))
length(which(st_is_valid(supermarkets) == FALSE))
length(which(st_is_valid(clinic) == FALSE))
length(which(st_is_valid(childcare) == FALSE))
length(which(st_is_valid(pre_school) == FALSE))
length(which(st_is_valid(bus) == FALSE))
```

##### Note: mrt has 2 entries of invalid geometries

```{r}
mrt <- st_make_valid(mrt)
```

##### Note: all data has been assigned correctly at EPSG 3414.

### Calculate Proximity

Create get_prox function to calculate proximity

1)  Calculate a matrix of distances between HDB and locational factors

2)  Get nearest point of locational factor by looking at minimum distance

3)  Rename column name according to input

```{r}
get_prox <- function(origin_df, dest_df, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)           
  
  # find the nearest location_factor and create new data frame
  near <- origin_df %>% 
    mutate(PROX = apply(dist_matrix, 1, function(x) min(x)) / 1000) 
  
  # rename column name according to input parameter
  names(near)[names(near) == 'PROX'] <- col_name

  # Return df
  return(near)
}

```

Calling get_prox function

Retrieve proximity of resale HDB and locational factors (eldercare, MRT, hawker, parks, clinics, supermarkets)

```{r}
rs_coords_sf <- get_prox(rs_coords_sf, eldercare, "PROX_ELDERLYCARE") 
rs_coords_sf <- get_prox(rs_coords_sf, mrt, "PROX_MRT") 
rs_coords_sf <- get_prox(rs_coords_sf, hawker, "PROX_HAWKER") 
rs_coords_sf <- get_prox(rs_coords_sf, parks, "PROX_PARK") 
rs_coords_sf <- get_prox(rs_coords_sf, supermarkets, "PROX_SUPERMARKET")
rs_coords_sf <- get_prox(rs_coords_sf, clinic, "PROX_CLINIC")
rs_coords_sf <- get_prox(rs_coords_sf, mall_sf, "PROX_MALL")
```

Creating get_within function to calculate the number of factors within a specific distance

1)  Calculate a matrix of distances between HDB and locational factors

2)  Get sum of points of locational factor within **threshold** distance

3)  Rename column name according to input to have distinct names

```{r}
get_within <- function(origin_df, dest_df, threshold_dist, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)   
  
  # count the number of location_factors within threshold_dist and create new data frame
  wdist <- origin_df %>% 
    mutate(WITHIN_DT = apply(dist_matrix, 1, function(x) sum(x <= threshold_dist)))
  
  # rename column name according to input parameter
  names(wdist)[names(wdist) == 'WITHIN_DT'] <- col_name

  # Return df
  return(wdist)
}
```

Calling get_within function

Retrieve number of locational factors that are within a certain threshold distance of 350m

```{r}
rs_coords_sf <- get_within(rs_coords_sf, pre_school, 350, "WITHIN_350M_PRESCHOOL")
rs_coords_sf <- get_within(rs_coords_sf, childcare, 350, "WITHIN_350M_CHILDCARE")
rs_coords_sf <- get_within(rs_coords_sf, bus, 350, "WITHIN_350M_BUS")
```

### Locational Data without Geographical Coordinates

#### CBD

We are required to retrieve the proximity of HDB resale flats to CBD area. With the help of Google, we have found that the latitude and longitude of Downtown Core also known as CBD, are 1.287953 and 103.851784 respectively.

As we already have the geographical coordinates of the resale data, we just need to convert the longitude and latitude of the CBD area to EPSG 3414 format before running the get_prox function.

Firstly, we create a dataframe consisting of the longitude and latitude coordinates of the CBD area, then transform to EPSG 3414 format.

##### Store CBD coordinates in a dataframe

```{r}
name <- c('CBD Area')
latitude= c(1.287953)
longitude= c(103.851784)
cbd_coords <- data.frame(name, latitude, longitude)
```

##### Assign and transform CRS

```{r}
cbd_coords_sf <- st_as_sf(cbd_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

```{r}
st_crs(cbd_coords_sf)
```

###### Note: CBD sf is now in EPSG 3414 format.

##### Call get_prox function

```{r}
rs_coords_sf <- get_prox(rs_coords_sf, cbd_coords_sf, "PROX_CBD")
```

#### Primary Schools

```{r}
pri_sch <- read_csv("data/geospatial/primaryschool/general-information-of-schools-2019-02-01T08-07-49Z.csv")
```

##### Extract relevant columns

```{r}
pri_sch <- pri_sch %>%
  filter(mainlevel_code == "PRIMARY") %>%
  select(school_name, address, postal_code, mainlevel_code)
```

```{r}
glimpse(pri_sch)
```

##### Create a list to store unique postal codes of primary schools

```{r}
prisch_list <- sort(unique(pri_sch$postal_code))
```

##### Call get_coords to retrieve coordinates of primary schools

```{r}
prisch_coords <- get_coords(prisch_list)
```

##### Inspect to see if there are any NA values

```{r}
prisch_coords[(is.na(prisch_coords$postal) | is.na(prisch_coords$latitude) | is.na(prisch_coords$longitude)),]
```

###### Note: 2 primary schools with no postal code, longitude and latitude:

-   228091: St. Margaret's Primary SchooL
-   319133: Pei Chun Public School

This could be because the postal code and address in the data does not match with the addresses in the API. Hence, we need to manually input as follows.

```{r}
#Pei Chun Public School
pri_sch[pri_sch$school_name == "PEI CHUN PUBLIC SCHOOL", "address"] <- "16 LORONG 7 TOA PAYOH"
pri_sch[pri_sch$school_name == "PEI CHUN PUBLIC SCHOOL", "postal_code"] <- "319320"

#St. Margaret's Primary School
pri_sch[pri_sch$school_name == "ST. MARGARET'S PRIMARY SCHOOL", "address"] <- "2 MATTAR ROAD"
pri_sch[pri_sch$school_name == "ST. MARGARET'S PRIMARY SCHOOL", "postal_code"] <- "387724"
```

##### Re-update dataframe

```{r}
prisch_list <- sort(unique(pri_sch$postal_code))
```

```{r}
prisch_coords <- get_coords(prisch_list)
```

```{r}
prisch_coords[(is.na(prisch_coords$postal) | is.na(prisch_coords$latitude) | is.na(prisch_coords$longitude)),]
```

Great, we have a full dataset now.

##### Combine coordinates and primary school names

```{r}
prisch_coords = prisch_coords[c("postal","latitude", "longitude")]
pri_sch <- left_join(pri_sch, prisch_coords, by = c('postal_code' = 'postal'))
```

##### Convert pri_sch dataframe into an sf object and transform CRS

```{r}
prisch_sf <- st_as_sf(pri_sch,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)

st_crs(prisch_sf)
```

##### Call get_within function

Define threshold distance to be 1km.

```{r}
rs_coords_sf <- get_within(rs_coords_sf, prisch_sf, 1000, "WITHIN_1KM_PRISCH")
```

### Good Primary Schools

Since we do not have dataset of good primary schools, we manually extract it from [Salary.sg](https://www.salary.sg/2022/best-primary-schools-2022-by-popularity/)

##### Store schools in a dataframe

```{r}
#creating a dataframe to store the schools
schools <- c("METHODIST GIRLS' SCHOOL (PRIMARY)",
                 "CATHOLIC HIGH SCHOOL",
                 "TAO NAN SCHOOL",
                 "PEI HWA PRESBYTERIAN PRIMARY SCHOOL",
                 "HOLY INNOCENTS' PRIMARY SCHOOL",
                 "NAN HUA PRIMARY SCHOOL",
                 "CHIJ SAINT. NICHOLAS GIRLS' SCHOOL",
                 "ADMIRALTY PRIMARY SCHOOL",
                 "SAINT. HILDA'S PRIMARY SCHOOL",
                 "AI TONG SCHOOL")
top_good_pri <- data.frame(schools)
```

##### Getting coordinates of the schools

```{r}
good_pri_list <- unique(top_good_pri$schools)
```

###### Call get_coords function

```{r}
goodprisch_coords <- get_coords(good_pri_list)
```

##### Inspect output

```{r}
goodprisch_coords[(is.na(goodprisch_coords$postal) | is.na(goodprisch_coords$latitude) | is.na(goodprisch_coords$longitude)), ]
```

Great, no NA values.

##### Convert goodpri_sch dataframe into an sf object and transform CRS

```{r}
goodpri_sf <- st_as_sf(goodprisch_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
st_crs(goodpri_sf)
```

##### Call get_prox to retrieve proximity of HDB flats and good primary schools

```{r}
rs_coords_sf <- get_prox(rs_coords_sf, goodpri_sf, "PROX_GOOD_PRISCH")
```

### Writing into rds file

```{r}
rs_factors_rds <- write_rds(rs_coords_sf, "data/rds/rs_factors.rds")
```

### Resale and locational factors

#### Read RDS file

```{r}
rs_sf <- read_rds("data/rds/rs_factors.rds")
```

##### Observations:

-   'Storey range' column is a string, hence called a categorical variable.
-   Categorical variables are unable to enter into regression equation.
-   Hence, we need to recode them into a series of variables so that they can enter the equation.
-   We will use dummy encoding to create dichotomous variables where each level of the categorical variable
-   However, some categorical variables have levels that are ordered. So, they can be converted to numerical variable instead and used as it is.
-   Here, storey_range can be ranked from low to high, which is a good way to provide us insights as to whether storey levels affect resale prices.
-   Therefore, we sort the variable and assign numerical values that range from low to high.

##### Extract unique storey_range and sort

```{r}
storeys <- sort(unique(rs_sf$storey_range))
```

##### Create new dataframe to store order of storey_range

```{r}
storey_order <- 1:length(storeys)
storey_range_order <- data.frame(storeys, storey_order)

head(storey_range_order)
```

###### Note: the storey ranges are rated 1-6 and are in the correct type to be used in the regression equation later on.

##### Combine storey_order with resale dataframe

```{r}
rs_sf <- left_join(rs_sf, storey_range_order, by= c("storey_range" = "storeys"))
```

##### Select required columns for analysis

```{r}
rs_req <- rs_sf %>%
  select(month, resale_price, floor_area_sqm, storey_order, remaining_lease_mths,
         PROX_CBD, PROX_ELDERLYCARE, PROX_HAWKER, PROX_MRT, PROX_PARK, PROX_GOOD_PRISCH, PROX_MALL, PROX_CLINIC,
         PROX_SUPERMARKET, WITHIN_350M_PRESCHOOL, WITHIN_350M_CHILDCARE, WITHIN_350M_BUS, WITHIN_1KM_PRISCH)
```

```{r}
summary(rs_req)
```

##### Write into rds file

```{r}
resale_final <- write_rds(rs_req, "data/rds/resale_final.rds")
```

### Import geospatial data

```{r}
mpsz <- st_read(dsn = "data/geospatial/national_boundary", layer="MP14_SUBZONE_WEB_PL")
```

```{r}
st_crs(mpsz)
```

##### Change to EPSG code 3414

```{r}
mpsz <- st_transform(mpsz, 3414)
st_crs(mpsz)
```

##### Check for invalid geometry

```{r}
length(which(st_is_valid(mpsz) == FALSE))
```

##### Make geometries valid

```{r}
mpsz <- st_make_valid(mpsz)
length(which(st_is_valid(mpsz) == FALSE))
```

##### Reveal extent of mpsz

```{r}
st_bbox(mpsz)
```

### Exploratory Data Analysis (EDA

##### Plot resale_price histogram

```{r}
ggplot(data=rs_req, aes(x=`resale_price`)) +
  geom_histogram(bins=20, color="black", fill="light green")
```

#### Inferences:

Right skewed distribution which suggests that more units are transacted at lower relative prices. Statistically, skewed distribution can be normalised by log transformation

#### Normalising using Log Transformation

##### Derive a new variable called log_resale_price by using log transformation on resale_price

```{r}
rs_req <- rs_req %>%
  mutate(`LOG_SELLING_PRICE` = log(resale_price))
```

#### Plot log_resale_price histogram

```{r}
ggplot(data=rs_req, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

Now, the distribution is less skewed. However, we will not be using this log resale price in our model due to the risk of high correlation with actual resale price.

## Predictive Modelling

#### Reading data

```{r}
resale_final <- read_rds("data/rds/resale_final.rds")
```

#### Data Sampling

We need to split our data into training and testing respectively. Training - 1 January 2021 to 31 December 2022 Testing - 1 January 2023 to 31 February 2023

```{r}
train_data <- resale_final %>% filter(month >= "2021-01" & month <= "2022-12")
test_data <- resale_final %>% filter(month >= "2023-01" & month <= "2023-02")
```

#### Write them into rds files respectively

```{r}
write_rds(train_data, "data/rds/train.rds")
write_rds(test_data, "data/rds/test.rds")
```

### Visualise relationships of independent variables

### Scatterplot Matrix

```{r}
resale_nogeo <- resale_final %>%
  st_drop_geometry()
corrplot(cor(resale_nogeo[, 2:17]), diag = FALSE, order = "AOE",
          tl.pos = "td", tl.cex = 0.8, method = "number", type = "upper")
```

##### Inferences:

Resale flats within 350m of childcare and pre-school are highly correlated at 0.91. This could be because

## Hedonic Pricing Model Using Multiple Linear Regression Method

```{r}
price_mlr <- lm(resale_price ~ floor_area_sqm +
                  storey_order + remaining_lease_mths +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + PROX_GOOD_PRISCH +
                  PROX_SUPERMARKET + WITHIN_350M_PRESCHOOL + WITHIN_350M_CHILDCARE +
                  WITHIN_350M_BUS + WITHIN_1KM_PRISCH,
                data=train_data)
```

```{r}
write_rds(price_mlr, "data/rds/price_mlr.rds")
```

```{r}
price_mlr <- read_rds("data/rds/price_mlr.rds")
```

```{r}
pred_mlr <- predict.lm(price_mlr, test_data)
```

```{r}
pred_mlr <- write_rds(pred_mlr, "data/rds/pred_mlr.rds")
```

##### Pass output into dateframe

```{r}
pred_mlr <- read_rds("data/rds/pred_mlr.rds")
pred_mlr_df <- as.data.frame(pred_mlr)

test_data_p_mlr <- cbind(test_data, pred_mlr_df)
```

## Visualisation

```{r}
ggplot(data = test_data_p_mlr,
       aes(x = pred_mlr,
           y = resale_price)) +
  geom_point()
```

## Building Geographical Weighted Random Forest Model

##### Retrieve stored data

```{r}
train <- read_rds("data/rds/train.rds")
test <- read_rds("data/rds/test.rds")
```

### Reducing data set size to Oct-Dec 2022

```{r}
train_data <- train_data %>% filter(month >= "2022-10" & month <= "2022-12")
```

##### Save as rds file

```{r}
write_rds(train_data, "data/rds/train_data_final.rds")
```

### Retrieve training data

```{r}
train_data <- read_rds("data/rds/train_data_final.rds")
```

```{r}
train_data_sp <- as_Spatial(train_data)
train_data_sp
```

## Computing Adaptive Bandwidth

```{r}
bw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +
                  storey_order + remaining_lease_mths +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_MALL + 
                  PROX_SUPERMARKET + WITHIN_350M_PRESCHOOL + WITHIN_350M_CHILDCARE +
                  WITHIN_350M_BUS + WITHIN_1KM_PRISCH,
                  data=train_data_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```

![](images/photo_2023-03-26_20-34-22.jpg)

##### Inference:34 neighbour points is the recommended bandwidth to use for this method of bandwidth calculation

```{r}
write_rds(bw_adaptive, "data/rds/bw_adaptive.rds")
```

### Coordinates Data

```{r}
coords <- st_coordinates(resale_final)
train_coords <- st_coordinates(train_data)
test_coords <- st_coordinates(test)
```

```{r}
train_coords <- write_rds(train_coords, "data/rds/train_coords.rds" )
test_coords <- write_rds(test_coords, "data/rds/test_coords.rds" )
```

#### Retrieve coords data

```{r}
train_coords <- read_rds("data/rds/train_coords.rds")
test_coords <- read_rds("data/rds/test_coords.rds")
```

```{r}
train_data <- train_data %>% 
  st_drop_geometry()
```

```{r}
train
```

## Calibrating Geographical Random Forest Model

```{r}
# set.seed(1234)
# gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + storey_order +
#                       remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE +
#                       PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +
#                       PROX_SUPERMARKET + WITHIN_350M_PRESCHOOL + WITHIN_350M_CHILDCARE +
#                       WITHIN_350M_BUS + WITHIN_1KM_PRISCH,
#                     dframe=train_data, 
#                     bw=bw_adaptive,
#                     kernel="adaptive",
#                     coords=train_coords,
#                     ntree = 30,
#                     min.node.size = 20)
```

### Save model output

```{r}
# write_rds(gwRF_adaptive, "data/rds/gwRF_adaptive.rds")
```

## Predicting using test data

```{r}
#test <- cbind(test, test_coords) %>%
#  st_drop_geometry()
```

```{r}
#gwRF_pred <- predict.grf(gwRF_adaptive,
#                           test_data,
#                           x.var.name="X",
#                           y.var.name="Y",
#                           local.w=1,
#                           global.w=0)
```

```{r}
# GRF_pred <- write_rds(gwRF_pred, "data/rds/GRF_pred.rds")
```

### Convert output into dataframe
```{r}
# GRF_pred <- read_rds("data/rds/GRF_pred.rds")
# GRF_pred_df <- as.data.frame(GRF_pred)
```
```{r}
# predicted_test_data <- cbind(test, GRF_pred_df) 
```

```{r}
# write_rds(predicted_test_data, "data/rds/predicted_test_data.rds")
```

### Root Mean Squared Error

```{r}
# Metrics::rmse(predicted_test_data$resale_price,
#     predicted_test_data$GRF_pred)
```

## Visualisation of Predicted Values

```{r}
#ggplot(data = predicted_test_data,
#       aes(x = GRF_pred,
#           y = resale_price)) +
#  geom_point()
```

## Conclusion: 
Geographical Weighted Random Forest Predictive Model is a better model than Ordinary Least Square method due to lower Root Mean Squared Error value.